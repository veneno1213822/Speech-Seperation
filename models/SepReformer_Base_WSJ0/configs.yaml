project: "[Project] SepReformer" ### Dont't change
notes: "SepReformer final version" ### Insert schanges(plz write details !!!)
# ------------------------------------------------------------------------------------------------------------------------------ #
config:
    # ------------------------------------------------------------ #
    dataset:
        max_len: 32000
        sampling_rate: 8000
        scp_dir: "data/scp_ss_8k"
        train:
            mixture: "tr_mix.scp"
            spk1: "tr_s1.scp"
            spk2: "tr_s2.scp"
            dynamic_mixing: false
        valid:
            mixture: "cv_mix.scp"
            spk1: "cv_s1.scp"
            spk2: "cv_s2.scp"
        test:
            mixture: "tt_mix.scp"
            spk1: "tt_s1.scp"
            spk2: "tt_s2.scp"
    # ------------------------------------------------------------ #
    dataloader:
        batch_size: 2
        pin_memory: false
        num_workers: 12
        drop_last: false
    # ------------------------------------------------------------ #
    model:
        num_stages: &var_model_num_stages 4 # R
        num_spks: &var_model_num_spks 2
        module_audio_enc:
            in_channels: 1
            out_channels: &var_model_audio_enc_out_channels 256
            kernel_size: &var_model_audio_enc_kernel_size 16 # L
            stride: &var_model_audio_enc_stride 4 # S
            groups: 1
            bias: false
        module_feature_projector:
            num_channels: *var_model_audio_enc_out_channels 
            in_channels: *var_model_audio_enc_out_channels
            out_channels: &feature_projector_out_channels 128 # F
            kernel_size: 1
            bias: false
        module_separator:
            num_stages: *var_model_num_stages
            relative_positional_encoding:
                in_channels: *feature_projector_out_channels
                num_heads: 8
                maxlen: 2000
                embed_v: false
            enc_stage:
                global_blocks:
                    in_channels: *feature_projector_out_channels
                    num_mha_heads: 8
                    dropout_rate: 0.05
                local_blocks:
                    in_channels: *feature_projector_out_channels
                    kernel_size: 65
                    dropout_rate: 0.05
                down_conv_layer:
                    in_channels: *feature_projector_out_channels
                    samp_kernel_size: &var_model_samp_kernel_size 5
            spk_split_stage:
                in_channels: *feature_projector_out_channels
                num_spks: *var_model_num_spks
            simple_fusion:
                out_channels: *feature_projector_out_channels
            dec_stage:
                num_spks: *var_model_num_spks
                global_blocks:
                    in_channels: *feature_projector_out_channels
                    num_mha_heads: 8
                    dropout_rate: 0.05
                local_blocks:
                    in_channels: *feature_projector_out_channels
                    kernel_size: 65
                    dropout_rate: 0.05
                spk_attention:
                    in_channels: *feature_projector_out_channels
                    num_mha_heads: 8
                    dropout_rate: 0.05
        module_output_layer:
            in_channels: *var_model_audio_enc_out_channels
            out_channels: *feature_projector_out_channels
            num_spks: *var_model_num_spks
        module_audio_dec:
            in_channels: *var_model_audio_enc_out_channels
            out_channels: 1
            kernel_size: *var_model_audio_enc_kernel_size
            stride: *var_model_audio_enc_stride
            bias: false
    # ------------------------------------------------------------ #
    criterion:
        name: ["PIT_SISNR_mag", "PIT_SISNR_time", "PIT_SISNRi", "PIT_SDRi"] ### Choose a torch.nn's loss function class(=attribute) e.g. ["L1Loss", "MSELoss", "CrossEntropyLoss", ...]
        PIT_SISNR_mag:
            frame_length: 512
            frame_shift: 128
            window: 'hann'
            num_stages: *var_model_num_stages
            num_spks: *var_model_num_spks
            scale_inv: true
            mel_opt: false
        PIT_SISNR_time:
            num_spks: *var_model_num_spks
            scale_inv: true
        PIT_SISNRi:
            num_spks: *var_model_num_spks
            scale_inv: true
        PIT_SDRi:
            dump: 0
    # ------------------------------------------------------------ #
    optimizer:
        name: ["AdamW"] ### Choose a torch.optim's class(=attribute) e.g. ["Adam", "AdamW", "SGD", ...]
        AdamW:
            lr: 1.0e-3
            weight_decay: 1.0e-2
    # ------------------------------------------------------------ #
    scheduler:
        name: ["ReduceLROnPlateau", "WarmupConstantSchedule"] ### Choose a torch.optim.lr_scheduler's class(=attribute) e.g. ["StepLR", "ReduceLROnPlateau", "Custom"]
        ReduceLROnPlateau:
            mode: "min"
            min_lr: 1.0e-10
            factor: 0.8
            patience: 2
        WarmupConstantSchedule:
            warmup_steps: 1000
    # ------------------------------------------------------------ #
    check_computations:
        dummy_len: 16000
    # ------------------------------------------------------------ #
    engine:
        max_epoch: 200
        gpuid: "0" ### "0"(single-gpu) or "0, 1" (multi-gpu)
        mvn: false
        clip_norm: 5
        start_scheduling: 50
        test_epochs: [100, 120, 150, 170]